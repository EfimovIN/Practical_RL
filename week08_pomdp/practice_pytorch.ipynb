{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\"bash\" ­Ґ пў«пҐвбп ў­гваҐ­­Ґ© Ё«Ё ў­Ґи­Ґ©\n",
      "Є®¬ ­¤®©, ЁбЇ®«­пҐ¬®© Їа®Ја ¬¬®© Ё«Ё Ї ЄҐв­л¬ д ©«®¬.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function, division\n",
    "from IPython.core import display\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "\n",
    "# if you're running in colab\n",
    "# !wget https://raw.githubusercontent.com/yandexdataschool/Practical_RL/0ccb0673965dd650d9b284e1ec90c2bfd82c8a94/week08_pomdp/atari_util.py\n",
    "# !wget https://raw.githubusercontent.com/yandexdataschool/Practical_RL/0ccb0673965dd650d9b284e1ec90c2bfd82c8a94/week08_pomdp/env_pool.py\n",
    "\n",
    "# If you are running on a server, launch xvfb to record game videos\n",
    "# Please make sure you have xvfb installed\n",
    "import os\n",
    "if type(os.environ.get(\"DISPLAY\")) is not str or len(os.environ.get(\"DISPLAY\")) == 0:\n",
    "    !bash ../xvfb start\n",
    "    os.environ['DISPLAY'] = ':1'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kung-Fu, recurrent style\n",
    "\n",
    "In this notebook we'll once again train RL agent for for atari [KungFuMaster](https://gym.openai.com/envs/KungFuMaster-v0/), this time using recurrent neural networks.\n",
    "\n",
    "![http://www.retroland.com/wp-content/uploads/2011/07/King-Fu-Master.jpg](http://www.retroland.com/wp-content/uploads/2011/07/King-Fu-Master.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observation shape: (1, 42, 42)\n",
      "Num actions: 14\n",
      "Action names: ['NOOP', 'UP', 'RIGHT', 'LEFT', 'DOWN', 'DOWNRIGHT', 'DOWNLEFT', 'RIGHTFIRE', 'LEFTFIRE', 'DOWNFIRE', 'UPRIGHTFIRE', 'UPLEFTFIRE', 'DOWNRIGHTFIRE', 'DOWNLEFTFIRE']\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "from atari_util import PreprocessAtari\n",
    "\n",
    "\n",
    "def make_env():\n",
    "    env = gym.make(\"KungFuMasterDeterministic-v0\")\n",
    "    env = PreprocessAtari(env, height=42, width=42,\n",
    "                          crop=lambda img: img[60:-30, 15:],\n",
    "                          color=False, n_frames=1)\n",
    "    return env\n",
    "\n",
    "\n",
    "env = make_env()\n",
    "\n",
    "obs_shape = env.observation_space.shape\n",
    "n_actions = env.action_space.n\n",
    "\n",
    "print(\"Observation shape:\", obs_shape)\n",
    "print(\"Num actions:\", n_actions)\n",
    "print(\"Action names:\", env.env.env.get_action_meanings())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "s = env.reset()\n",
    "for _ in range(100):\n",
    "    s, _, _, _ = env.step(env.action_space.sample())\n",
    "\n",
    "plt.title('Game image')\n",
    "plt.imshow(env.render('rgb_array'))\n",
    "plt.show()\n",
    "\n",
    "plt.title('Agent observation')\n",
    "plt.imshow(s.reshape([42, 42]))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### POMDP setting\n",
    "\n",
    "The atari game we're working with is actually a POMDP: your agent needs to know timing at which enemies spawn and move, but cannot do so unless it has some memory. \n",
    "\n",
    "Let's design another agent that has a recurrent neural net memory to solve this. Here's a sketch.\n",
    "\n",
    "![img](img1.jpg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# a special module that converts [batch, channel, w, h] to [batch, units]\n",
    "\n",
    "\n",
    "class Flatten(nn.Module):\n",
    "    def forward(self, input):\n",
    "        return input.view(input.size(0), -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleRecurrentAgent(nn.Module):\n",
    "    def __init__(self, obs_shape, n_actions, reuse=False):\n",
    "        \"\"\"A simple actor-critic agent\"\"\"\n",
    "        super(self.__class__, self).__init__()\n",
    "\n",
    "        self.conv0 = nn.Conv2d(1, 32, kernel_size=(3, 3), stride=(2, 2))\n",
    "        self.conv1 = nn.Conv2d(32, 32, kernel_size=(3, 3), stride=(2, 2))\n",
    "        self.conv2 = nn.Conv2d(32, 32, kernel_size=(3, 3), stride=(2, 2))\n",
    "        self.flatten = Flatten()\n",
    "\n",
    "        self.hid = nn.Linear(512, 128)\n",
    "        self.rnn = nn.LSTMCell(128, 128)\n",
    "\n",
    "        self.logits = nn.Linear(128, n_actions)\n",
    "        self.state_value = nn.Linear(128, 1)\n",
    "\n",
    "    def forward(self, prev_state, obs_t):\n",
    "        \"\"\"\n",
    "        Takes agent's previous step and observation, \n",
    "        returns next state and whatever it needs to learn (tf tensors)\n",
    "        \"\"\"\n",
    "\n",
    "        # YOUR CODE: apply the whole neural net for one step here.\n",
    "        # See docs on self.rnn(...)\n",
    "        # the recurrent cell should take the last feedforward dense layer as input\n",
    "        h = self.conv0(obs_t)\n",
    "        h = F.elu(h)\n",
    "        h = self.conv1(h)\n",
    "        h = F.elu(h)\n",
    "        h = self.conv2(h)\n",
    "        \n",
    "        h = h.view(h.shape[0], -1)\n",
    "        h = self.hid(h)\n",
    "        h = F.relu(h)\n",
    "        \n",
    "        \n",
    "        \n",
    "        new_state = h_new, c_new = self.rnn(h, prev_state)\n",
    "        logits = self.logits(h_new)\n",
    "        state_value = self.state_value(h_new)\n",
    "\n",
    "        return new_state, (logits, state_value)\n",
    "\n",
    "    def get_initial_state(self, batch_size):\n",
    "        \"\"\"Return a list of agent memory states at game start. Each state is a np array of shape [batch_size, ...]\"\"\"\n",
    "        return torch.zeros((batch_size, 128)), torch.zeros((batch_size, 128))\n",
    "\n",
    "    def sample_actions(self, agent_outputs):\n",
    "        \"\"\"pick actions given numeric agent outputs (np arrays)\"\"\"\n",
    "        logits, state_values = agent_outputs\n",
    "        probs = F.softmax(logits)\n",
    "        return torch.multinomial(probs, 1)[:, 0].data.numpy()\n",
    "\n",
    "    def step(self, prev_state, obs_t):\n",
    "        \"\"\" like forward, but obs_t is a numpy array \"\"\"\n",
    "        obs_t = torch.tensor(np.asarray(obs_t), dtype=torch.float32)\n",
    "        (h, c), (l, s) = self.forward(prev_state, obs_t)\n",
    "        return (h.detach(), c.detach()), (l.detach(), s.detach())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_parallel_games = 5\n",
    "gamma = 0.99\n",
    "\n",
    "agent = SimpleRecurrentAgent(obs_shape, n_actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "action logits:\n",
      " tensor([[ 0.0577,  0.0003, -0.0161,  0.0530, -0.0069, -0.0341, -0.0392,  0.0746,\n",
      "          0.0022,  0.0891,  0.0595, -0.0631,  0.0123,  0.0367]])\n",
      "state values:\n",
      " tensor([[-0.0858]])\n"
     ]
    }
   ],
   "source": [
    "state = [env.reset()]\n",
    "_, (logits, value) = agent.step(agent.get_initial_state(1), state)\n",
    "print(\"action logits:\\n\", logits)\n",
    "print(\"state values:\\n\", value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's play!\n",
    "Let's build a function that measures agent's average reward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(agent, env, n_games=1):\n",
    "    \"\"\"Plays an entire game start to end, returns session rewards.\"\"\"\n",
    "\n",
    "    game_rewards = []\n",
    "    for _ in range(n_games):\n",
    "        # initial observation and memory\n",
    "        observation = env.reset()\n",
    "        prev_memories = agent.get_initial_state(1)\n",
    "\n",
    "        total_reward = 0\n",
    "        while True:\n",
    "            new_memories, readouts = agent.step(\n",
    "                prev_memories, observation[None, ...])\n",
    "            action = agent.sample_actions(readouts)\n",
    "\n",
    "            observation, reward, done, info = env.step(action[0])\n",
    "\n",
    "            total_reward += reward\n",
    "            prev_memories = new_memories\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "        game_rewards.append(total_reward)\n",
    "    return game_rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:51: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[400.0, 700.0, 200.0]\n"
     ]
    }
   ],
   "source": [
    "env_monitor = gym.wrappers.Monitor(env, directory=\"kungfu_videos\", force=True)\n",
    "rw = evaluate(agent, env_monitor, n_games=3,)\n",
    "env_monitor.close()\n",
    "print(rw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<video width=\"640\" height=\"480\" controls>\n",
       "  <source src=\"./kungfu_videos/openaigym.video.0.12972.video000001.mp4\" type=\"video/mp4\">\n",
       "</video>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# show video\n",
    "from IPython.display import HTML\n",
    "import os\n",
    "\n",
    "video_names = list(filter(lambda s: s.endswith(\n",
    "    \".mp4\"), os.listdir(\"./kungfu_videos/\")))\n",
    "\n",
    "HTML(\"\"\"\n",
    "<video width=\"640\" height=\"480\" controls>\n",
    "  <source src=\"{}\" type=\"video/mp4\">\n",
    "</video>\n",
    "\"\"\".format(\"./kungfu_videos/\"+video_names[-1]))  # this may or may not be _last_ video. Try other indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training on parallel games\n",
    "\n",
    "We introduce a class called EnvPool - it's a tool that handles multiple environments for you. Here's how it works:\n",
    "![img](img2.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from env_pool import EnvPool\n",
    "pool = EnvPool(agent, make_env, n_parallel_games)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We gonna train our agent on a thing called __rollouts:__\n",
    "![img](img3.jpg)\n",
    "\n",
    "A rollout is just a sequence of T observations, actions and rewards that agent took consequently.\n",
    "* First __s0__ is not necessarily initial state for the environment\n",
    "* Final state is not necessarily terminal\n",
    "* We sample several parallel rollouts for efficiency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:51: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    }
   ],
   "source": [
    "# for each of n_parallel_games, take 10 steps\n",
    "rollout_obs, rollout_actions, rollout_rewards, rollout_mask = pool.interact(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actions shape: (5, 10)\n",
      "Rewards shape: (5, 10)\n",
      "Mask shape: (5, 10)\n",
      "Observations shape:  (5, 10, 1, 42, 42)\n"
     ]
    }
   ],
   "source": [
    "print(\"Actions shape:\", rollout_actions.shape)\n",
    "print(\"Rewards shape:\", rollout_rewards.shape)\n",
    "print(\"Mask shape:\", rollout_mask.shape)\n",
    "print(\"Observations shape: \", rollout_obs.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Actor-critic objective\n",
    "\n",
    "Here we define a loss function that uses rollout above to train advantage actor-critic agent.\n",
    "\n",
    "\n",
    "Our loss consists of three components:\n",
    "\n",
    "* __The policy \"loss\"__\n",
    " $$ \\hat J = {1 \\over T} \\cdot \\sum_t { \\log \\pi(a_t | s_t) } \\cdot A_{const}(s,a) $$\n",
    "  * This function has no meaning in and of itself, but it was built such that\n",
    "  * $ \\nabla \\hat J = {1 \\over N} \\cdot \\sum_t { \\nabla \\log \\pi(a_t | s_t) } \\cdot A(s,a) \\approx \\nabla E_{s, a \\sim \\pi} R(s,a) $\n",
    "  * Therefore if we __maximize__ J_hat with gradient descent we will maximize expected reward\n",
    "  \n",
    "  \n",
    "* __The value \"loss\"__\n",
    "  $$ L_{td} = {1 \\over T} \\cdot \\sum_t { [r + \\gamma \\cdot V_{const}(s_{t+1}) - V(s_t)] ^ 2 }$$\n",
    "  * Ye Olde TD_loss from q-learning and alike\n",
    "  * If we minimize this loss, V(s) will converge to $V_\\pi(s) = E_{a \\sim \\pi(a | s)} R(s,a) $\n",
    "\n",
    "\n",
    "* __Entropy Regularizer__\n",
    "  $$ H = - {1 \\over T} \\sum_t \\sum_a {\\pi(a|s_t) \\cdot \\log \\pi (a|s_t)}$$\n",
    "  * If we __maximize__ entropy we discourage agent from predicting zero probability to actions\n",
    "  prematurely (a.k.a. exploration)\n",
    "  \n",
    "  \n",
    "So we optimize a linear combination of $L_{td}$ $- \\hat J$, $-H$\n",
    "  \n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "__One more thing:__ since we train on T-step rollouts, we can use N-step formula for advantage for free:\n",
    "  * At the last step, $A(s_t,a_t) = r(s_t, a_t) + \\gamma \\cdot V(s_{t+1}) - V(s) $\n",
    "  * One step earlier, $A(s_t,a_t) = r(s_t, a_t) + \\gamma \\cdot r(s_{t+1}, a_{t+1}) + \\gamma ^ 2 \\cdot V(s_{t+2}) - V(s) $\n",
    "  * Et cetera, et cetera. This way agent starts training much faster since it's estimate of A(s,a) depends less on his (imperfect) value function and more on actual rewards. There's also a [nice generalization](https://arxiv.org/abs/1506.02438) of this.\n",
    "\n",
    "\n",
    "__Note:__ it's also a good idea to scale rollout_len up to learn longer sequences. You may wish set it to >=20 or to start at 10 and then scale up as time passes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_one_hot(y, n_dims=None):\n",
    "    \"\"\" Take an integer tensor and convert it to 1-hot matrix. \"\"\"\n",
    "    y_tensor = y.to(dtype=torch.int64).view(-1, 1)\n",
    "    n_dims = n_dims if n_dims is not None else int(torch.max(y_tensor)) + 1\n",
    "    y_one_hot = torch.zeros(y_tensor.size()[0], n_dims).scatter_(1, y_tensor, 1)\n",
    "    return y_one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = torch.optim.Adam(agent.parameters(), lr=1e-4)\n",
    "\n",
    "\n",
    "def train_on_rollout(states, actions, rewards, is_not_done, prev_memory_states, gamma=0.99):\n",
    "    \"\"\"\n",
    "    Takes a sequence of states, actions and rewards produced by generate_session.\n",
    "    Updates agent's weights by following the policy gradient above.\n",
    "    Please use Adam optimizer with default parameters.\n",
    "    \"\"\"\n",
    "\n",
    "    # shape: [batch_size, time, c, h, w]\n",
    "    states = torch.tensor(np.asarray(states), dtype=torch.float32)\n",
    "    actions = torch.tensor(np.array(actions), dtype=torch.int64)  # shape: [batch_size, time]\n",
    "    rewards = torch.tensor(np.array(rewards), dtype=torch.float32)  # shape: [batch_size, time]\n",
    "    is_not_done = torch.tensor(np.array(is_not_done), dtype=torch.float32)  # shape: [batch_size, time]\n",
    "    rollout_length = rewards.shape[1] - 1\n",
    "\n",
    "    # predict logits, probas and log-probas using an agent.\n",
    "    memory = [m.detach() for m in prev_memory_states]\n",
    "\n",
    "    logits = []  # append logit sequence here\n",
    "    state_values = []  # append state values here\n",
    "    for t in range(rewards.shape[1]):\n",
    "        obs_t = states[:, t]\n",
    "\n",
    "        # use agent to comute logits_t and state values_t.\n",
    "        # append them to logits and state_values array\n",
    "\n",
    "        memory, (logits_t, values_t) = agent.forward(memory, obs_t)\n",
    "\n",
    "        logits.append(logits_t)\n",
    "        state_values.append(values_t)\n",
    "\n",
    "    logits = torch.stack(logits, dim=1)\n",
    "    state_values = torch.stack(state_values, dim=1)\n",
    "    probas = F.softmax(logits, dim=2)\n",
    "    logprobas = F.log_softmax(logits, dim=2)\n",
    "\n",
    "    # select log-probabilities for chosen actions, log pi(a_i|s_i)\n",
    "    actions_one_hot = to_one_hot(actions, n_actions).view(\n",
    "        actions.shape[0], actions.shape[1], n_actions)\n",
    "    logprobas_for_actions = torch.sum(logprobas * actions_one_hot, dim=-1)\n",
    "\n",
    "    # Now let's compute two loss components:\n",
    "    # 1) Policy gradient objective.\n",
    "    # Notes: Please don't forget to call .detach() on advantage term. Also please use mean, not sum.\n",
    "    # it's okay to use loops if you want\n",
    "    J_hat = 0  # policy objective as in the formula for J_hat\n",
    "\n",
    "    # 2) Temporal difference MSE for state values\n",
    "    # Notes: Please don't forget to call on V(s') term. Also please use mean, not sum.\n",
    "    # it's okay to use loops if you want\n",
    "    value_loss = 0\n",
    "\n",
    "    cumulative_returns = state_values[:, -1].detach()\n",
    "\n",
    "    for t in reversed(range(rollout_length)):\n",
    "        r_t = rewards[:, t]                                # current rewards\n",
    "        # current state values\n",
    "        V_t = state_values[:, t]\n",
    "        V_next = state_values[:, t + 1].detach()           # next state values\n",
    "        # log-probability of a_t in s_t\n",
    "        logpi_a_s_t = logprobas_for_actions[:, t]\n",
    "\n",
    "        # update G_t = r_t + gamma * G_{t+1} as we did in week6 reinforce\n",
    "        cumulative_returns = G_t = r_t + gamma * cumulative_returns\n",
    "\n",
    "        # Compute temporal difference error (MSE for V(s))\n",
    "        value_loss += torch.mean((r_t +gamma*V_next-V_t)**2)\n",
    "\n",
    "        # compute advantage A(s_t, a_t) using cumulative returns and V(s_t) as baseline\n",
    "        advantage = cumulative_returns - V_t\n",
    "        advantage = advantage.detach()\n",
    "\n",
    "        # compute policy pseudo-loss aka -J_hat.\n",
    "        J_hat += torch.mean(logpi_a_s_t*advantage)\n",
    "\n",
    "    # regularize with entropy\n",
    "    entropy_reg = -torch.mean(torch.sum(probas*logprobas , dim=-1))\n",
    "\n",
    "    # add-up three loss components and average over time\n",
    "    loss = -J_hat / rollout_length +\\\n",
    "        value_loss / rollout_length +\\\n",
    "           -0.01 * entropy_reg\n",
    "\n",
    "    # Gradient descent step\n",
    "    opt.zero_grad()\n",
    "    loss.backward()\n",
    "    opt.step()\n",
    "\n",
    "    return loss.data.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:51: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array(-0.01639859, dtype=float32)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let's test it\n",
    "memory = list(pool.prev_memory_states)\n",
    "rollout_obs, rollout_actions, rollout_rewards, rollout_mask = pool.interact(10)\n",
    "\n",
    "train_on_rollout(rollout_obs, rollout_actions,\n",
    "                 rollout_rewards, rollout_mask, memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train \n",
    "\n",
    "just run train step and see if agent learns any better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "from tqdm import trange\n",
    "from pandas import DataFrame\n",
    "moving_average = lambda x, **kw: DataFrame(\n",
    "    {'x': np.asarray(x)}).x.ewm(**kw).mean().values\n",
    "\n",
    "rewards_history = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "  3%|██▌                                                                        | 501/15000 [05:05<17:11:23,  4.27s/it]\n",
      "\n",
      "  3%|██▌                                                                        | 502/15000 [05:05<12:26:56,  3.09s/it]\n",
      "\n",
      "  3%|██▌                                                                         | 503/15000 [05:05<9:15:00,  2.30s/it]\n",
      "\n",
      "  3%|██▌                                                                         | 504/15000 [05:06<7:01:22,  1.74s/it]\n",
      "\n",
      "  3%|██▌                                                                         | 505/15000 [05:06<5:26:06,  1.35s/it]\n",
      "\n",
      "  3%|██▌                                                                         | 506/15000 [05:07<4:19:16,  1.07s/it]\n",
      "\n",
      "  3%|██▌                                                                         | 507/15000 [05:07<3:36:02,  1.12it/s]\n",
      "\n",
      "  3%|██▌                                                                         | 508/15000 [05:08<3:04:28,  1.31it/s]\n",
      "\n",
      "  3%|██▌                                                                         | 509/15000 [05:08<2:44:41,  1.47it/s]\n",
      "\n",
      "  3%|██▌                                                                         | 510/15000 [05:09<2:26:39,  1.65it/s]\n",
      "\n",
      "  3%|██▌                                                                         | 511/15000 [05:09<2:15:45,  1.78it/s]\n",
      "\n",
      "  3%|██▌                                                                         | 512/15000 [05:10<2:14:08,  1.80it/s]\n",
      "\n",
      "  3%|██▌                                                                         | 513/15000 [05:10<2:07:43,  1.89it/s]\n",
      "\n",
      "  3%|██▌                                                                         | 514/15000 [05:11<2:03:30,  1.95it/s]\n",
      "\n",
      "  3%|██▌                                                                         | 515/15000 [05:11<2:02:31,  1.97it/s]\n",
      "\n",
      "  3%|██▌                                                                         | 516/15000 [05:11<2:00:18,  2.01it/s]"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-25-bc930d433b03>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0mmemory\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpool\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprev_memory_states\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     rollout_obs, rollout_actions, rollout_rewards, rollout_mask = pool.interact(\n\u001b[1;32m----> 5\u001b[1;33m         10)\n\u001b[0m\u001b[0;32m      6\u001b[0m     train_on_rollout(rollout_obs, rollout_actions,\n\u001b[0;32m      7\u001b[0m                      rollout_rewards, rollout_mask, memory)\n",
      "\u001b[1;32mD:\\Ilya\\Workshop\\Projects\\Practical_RL\\week08_pomdp\\env_pool.py\u001b[0m in \u001b[0;36minteract\u001b[1;34m(self, n_steps, verbose)\u001b[0m\n\u001b[0;32m     82\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     83\u001b[0m             new_observations, cur_rewards, is_alive, infos = zip(\n\u001b[1;32m---> 84\u001b[1;33m                 *map(env_step, range(len(self.envs)), actions))\n\u001b[0m\u001b[0;32m     85\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     86\u001b[0m             \u001b[1;31m# Append data tuple for this tick.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Ilya\\Workshop\\Projects\\Practical_RL\\week08_pomdp\\env_pool.py\u001b[0m in \u001b[0;36menv_step\u001b[1;34m(i, action)\u001b[0m\n\u001b[0;32m     48\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjust_ended\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     49\u001b[0m                 \u001b[0mnew_observation\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcur_reward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mis_done\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[1;33m=\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 50\u001b[1;33m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menvs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     51\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mis_done\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     52\u001b[0m                     \u001b[1;31m# Game ends now, will finalize on next tick.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Ilya\\Workshop\\Projects\\Practical_RL\\week08_pomdp\\atari_util.py\u001b[0m in \u001b[0;36mstep\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     33\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m         \u001b[1;34m\"\"\"plays breakout for 1 step, returns frame buffer\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 35\u001b[1;33m         \u001b[0mnew_img\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     36\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate_buffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnew_img\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mframebuffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minfo\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\gym\\wrappers\\time_limit.py\u001b[0m in \u001b[0;36mstep\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     14\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m         \u001b[1;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_elapsed_steps\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"Cannot call env.step() before calling reset()\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m         \u001b[0mobservation\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_elapsed_steps\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_elapsed_steps\u001b[0m \u001b[1;33m>=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_max_episode_steps\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\gym\\envs\\atari\\atari_env.py\u001b[0m in \u001b[0;36mstep\u001b[1;34m(self, a)\u001b[0m\n\u001b[0;32m    119\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnum_steps\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    120\u001b[0m             \u001b[0mreward\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0male\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mact\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 121\u001b[1;33m         \u001b[0mob\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_obs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    122\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    123\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mob\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0male\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgame_over\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;34m\"ale.lives\"\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0male\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlives\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\gym\\envs\\atari\\atari_env.py\u001b[0m in \u001b[0;36m_get_obs\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    137\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_ram\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    138\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_obs_type\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'image'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 139\u001b[1;33m             \u001b[0mimg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_image\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    140\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    141\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\gym\\envs\\atari\\atari_env.py\u001b[0m in \u001b[0;36m_get_image\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    124\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    125\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_get_image\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 126\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0male\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetScreenRGB2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    127\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    128\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_get_ram\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\atari_py\\ale_python_interface.py\u001b[0m in \u001b[0;36mgetScreenRGB2\u001b[1;34m(self, screen_data)\u001b[0m\n\u001b[0;32m    264\u001b[0m             \u001b[0mscreen_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mempty\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mheight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwidth\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0muint8\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    265\u001b[0m         \u001b[1;32massert\u001b[0m \u001b[0mscreen_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstrides\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m480\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 266\u001b[1;33m         \u001b[0male_lib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetScreenRGB2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mas_ctypes\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mscreen_data\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    267\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mscreen_data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    268\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for i in trange(15000):\n",
    "\n",
    "    memory = list(pool.prev_memory_states)\n",
    "    rollout_obs, rollout_actions, rollout_rewards, rollout_mask = pool.interact(\n",
    "        10)\n",
    "    train_on_rollout(rollout_obs, rollout_actions,\n",
    "                     rollout_rewards, rollout_mask, memory)\n",
    "\n",
    "    if i % 100 == 0:\n",
    "        rewards_history.append(np.mean(evaluate(agent, env, n_games=1)))\n",
    "        clear_output(True)\n",
    "        plt.plot(rewards_history, label='rewards')\n",
    "        plt.plot(moving_average(np.array(rewards_history),\n",
    "                                span=10), label='rewards ewma@10')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "        if rewards_history[-1] >= 10000:\n",
    "            print(\"Your agent has just passed the minimum homework threshold\")\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Relax and grab some refreshments while your agent is locked in an infinite loop of violence and death.\n",
    "\n",
    "__How to interpret plots:__\n",
    "\n",
    "The session reward is the easy thing: it should in general go up over time, but it's okay if it fluctuates ~~like crazy~~. It's also OK if it reward doesn't increase substantially before some 10k initial steps. However, if reward reaches zero and doesn't seem to get up over 2-3 evaluations, there's something wrong happening.\n",
    "\n",
    "\n",
    "Since we use a policy-based method, we also keep track of __policy entropy__ - the same one you used as a regularizer. The only important thing about it is that your entropy shouldn't drop too low (`< 0.1`) before your agent gets the yellow belt. Or at least it can drop there, but _it shouldn't stay there for long_.\n",
    "\n",
    "If it does, the culprit is likely:\n",
    "* Some bug in entropy computation. Remember that it is $ - \\sum p(a_i) \\cdot log p(a_i) $\n",
    "* Your agent architecture converges too fast. Increase entropy coefficient in actor loss. \n",
    "* Gradient explosion - just [clip gradients](https://stackoverflow.com/a/43486487) and maybe use a smaller network\n",
    "* Us. Or TF developers. Or aliens. Or lizardfolk. Contact us on forums before it's too late!\n",
    "\n",
    "If you're debugging, just run `logits, values = agent.step(batch_states)` and manually look into logits and values. This will reveal the problem 9 times out of 10: you'll likely see some NaNs or insanely large numbers or zeros. Try to catch the moment when this happens for the first time and investigate from there."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### \"Final\" evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:51: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    }
   ],
   "source": [
    "env_monitor = gym.wrappers.Monitor(env, directory=\"kungfu_videos\", force=True)\n",
    "final_rewards = evaluate(agent, env_monitor, n_games=20,)\n",
    "env_monitor.close()\n",
    "print(\"Final mean reward\", np.mean(final_rewards))\n",
    "\n",
    "video_names = list(filter(lambda s: s.endswith(\n",
    "    \".mp4\"), os.listdir(\"./kungfu_videos/\")))\n",
    "HTML(\"\"\"\n",
    "<video width=\"640\" height=\"480\" controls>\n",
    "  <source src=\"{}\" type=\"video/mp4\">\n",
    "</video>\n",
    "\"\"\".format(\"./kungfu_videos/\"+video_names[-1]))  # this may or may not be _last_ video. Try other indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
